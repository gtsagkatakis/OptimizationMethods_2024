{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5dbLHjDYf28NoCCYG8wUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtsagkatakis/OptimizationMethods_2024/blob/main/RidgeRegression_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y4AcbmXOpWW",
        "outputId": "fa10587d-85fc-4cb3-ca7f-885252ad0a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Error: 0.428, Testing Error: 2.604\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(100, 50)  # 100 samples, 10 feature\n",
        "y = 4 + 3 * X + np.random.randn(100, 50)  # y = 4 + 3X + noise\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Define functions for typical linear regression and ridge regression\n",
        "def typical_linear_regression(X, y):\n",
        "    # Closed-form solution for linear regression: (X^T X)^(-1) X^T y\n",
        "    w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "    return w\n",
        "\n",
        "\n",
        "# Typical Linear Regression\n",
        "w_typical = typical_linear_regression(X_train, y_train)\n",
        "y_train_pred_typical = X_train @ w_typical\n",
        "y_test_pred_typical = X_test @ w_typical\n",
        "train_error_typical = mean_squared_error(y_train, y_train_pred_typical)\n",
        "test_error_typical = mean_squared_error(y_test, y_test_pred_typical)\n",
        "\n",
        "print(\" Training Error: {:.3f}, Testing Error: {:.3f}\".format(train_error_typical, test_error_typical))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def ridge_regression(X, y, alpha):\n"
      ],
      "metadata": {
        "id": "8Tyqw_WlQFW1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate weights for both typical and ridge regression with different alpha values\n",
        "alpha_values = [0.1, 1, 10, 100]  # Different values of the regularization parameter\n",
        "train_errors_ridge = []\n",
        "test_errors_ridge = []\n",
        "ridge_coefs = []\n",
        "\n",
        "# Ridge Regression for Different Alphas\n",
        "for alpha in alpha_values:\n",
        "    # Train the ridge regression model\n",
        "    w_ridge = ridge_regression(X_train, y_train, alpha)\n",
        "\n",
        "    # Make predictions for train and test sets\n",
        "    y_train_pred_ridge = X_train @ w_ridge\n",
        "    y_test_pred_ridge = X_test @ w_ridge\n",
        "\n",
        "    # Calculate mean squared error for train and test sets\n",
        "    train_errors_ridge.append(mean_squared_error(y_train, y_train_pred_ridge))\n",
        "    test_errors_ridge.append(mean_squared_error(y_test, y_test_pred_ridge))\n",
        "    ridge_coefs.append(w_ridge[0][0])  # Save the coefficient to observe the effect of regularization\n",
        "\n",
        "# Plot train and test errors for typical and ridge regression\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Typical Linear Regression Errors (Solid lines)\n",
        "plt.axhline(y=train_error_typical, color=\"blue\", linestyle='-', label=\"Train Error (Typical)\")\n",
        "plt.axhline(y=test_error_typical, color=\"red\", linestyle='-', label=\"Test Error (Typical)\")\n",
        "\n",
        "# Ridge Regression Errors (Dashed lines with markers)\n",
        "plt.plot(alpha_values, train_errors_ridge, label=\"Train Error (Ridge)\", marker='o', linestyle='--', color=\"blue\")\n",
        "plt.plot(alpha_values, test_errors_ridge, label=\"Test Error (Ridge)\", marker='o', linestyle='--', color=\"red\")\n",
        "\n",
        "# Customize plot\n",
        "plt.xscale(\"log\")  # Log scale for better visualization\n",
        "plt.xlabel(\"Regularization Parameter (alpha)\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Effect of Regularization on Train and Test Error\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot coefficients for ridge regression as a function of alpha\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(alpha_values, ridge_coefs, marker='o', label=\"Ridge Coefficient\", linestyle='--')\n",
        "plt.axhline(y=w_typical[0][0], color='gray', linestyle='-', label=\"Coefficient (Typical)\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Regularization Parameter (alpha)\")\n",
        "plt.ylabel(\"Coefficient (w)\")\n",
        "plt.title(\"Effect of Regularization on Model Coefficient\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Typical Linear Regression - Training Error: {:.3f}, Testing Error: {:.3f}\".format(train_error_typical, test_error_typical))\n",
        "for i, alpha in enumerate(alpha_values):\n",
        "    print(\"Ridge Regression (alpha={:.1f}) - Training Error: {:.3f}, Testing Error: {:.3f}\".format(alpha, train_errors_ridge[i], test_errors_ridge[i]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5qutok3qQAx9",
        "outputId": "1bb8d5a9-1762-4517-b458-b6e2a8f1bcd7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ridge_regression' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c1581cbfa8b3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malpha_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Train the ridge regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mw_ridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Make predictions for train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ridge_regression' is not defined"
          ]
        }
      ]
    }
  ]
}